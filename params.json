{"name":"Using cuda-stream to concurrent device memory transmission and calculation","tagline":"","body":"### Welcome to GitHub Pages.\r\nSimple introduce of [cuda-stream](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/)      \r\n\r\nYou can get My code from here:   \r\n```\r\n$ git clone https://github.com/zhxfl/CUDA-CNN.git   \r\n```\r\n\r\nThe first version of my project was only designed for MNIST which is a small and simply dataset, so we copy all the data to device memory.  As the size of dataset increase, now I just want to keep \"batch size\" of the image on the device. Before every epoch I should copy the data from host memory to device memory, so I need the Cuda-Stream technology to concurrent device memory transmission and calculation. \r\n\r\nIn order to using cuda-streams, I need double buffers:\r\n```\r\ncuMatrixVector<double>batchImg[2];\r\n/*double buffer for batch images*/\r\n\tfor(int i = 0; i < 2; i ++){\r\n\t\tfor(int j = 0; j < batch; j++){\r\n\t\t\tbatchImg[i].push_back(new cuMatrix<double>(ImgSize + crop, ImgSize + crop, Config::instance()->getChannels()));\r\n\t\t}\r\n\t\tbatchImg[i].toGpu();\r\n\t}\r\n```\r\nHere is a function copy the host memory to device memory using cuda-streams\r\n\r\n```\r\nvoid getBatchImageWithStreams(cuMatrixVector<double>&x, cuMatrixVector<double>&batchImg, int start, cudaStream_t stream1){\r\n\t for(int i = 0; i < batchImg.size(); i++){\r\n\t\t memcpy(batchImg[i]->hostData, x[i + start]->hostData, sizeof(double) * batchImg[i]->getLen());\r\n\t\t batchImg[i]->toGpu(stream1);\r\n\t }\r\n}\r\n\r\n```\r\n\r\nNow I shows the way to use these double buffers.\r\n```\r\n1. getBatchImageWithStreams(x, batchImg[0], 0, stream1);\r\n2. int batchImgId = 1;\r\n3. for (int k = 0; k <= x.size() - batch; k += batch) {\r\n4.    cudaStreamSynchronize(stream1);\r\n5.    int start = k;\r\n6.    printf(\"train %2d%%\", 100 * k / ((x.size() + batch - 1)));\r\n7.    if(start + batch <= x.size() - batch)\r\n8. \tgetBatchImageWithStreams(x, batchImg[batchImgId], start + batch, stream1);\r\n9.    batchImgId = 1 - batchImgId;\r\n10. }\r\n```\r\nAs the code shows, line-4 synchronize the first buffer to device, than line-8 copy the next batch to the device. We can find the line-8's transfers will concurrent with the following kernel functions.\r\n\r\n# The result:\r\nNow the CIFAR-10 dataset can run in the computer with 2G device memory, the every epoch's running time decline frome 28 seconds to 24 seconds. \r\n\r\n### Support or Contact   \r\ncontact zhxfl@mail.ustc.edu.cn, I â€™ll help you sort it out.   ","google":"cuda cnn cuda-stream","note":"Don't delete this file! It's used internally to help with page regeneration."}